Wan2.1-T2V Quantization Evaluation

Reproducible FP16 vs TorchAO W8A16 (INT8 weight-only) evaluation pipeline for Wan2.1-T2V-1.3B on consumer GPUs.

æœ¬é¡¹ç›®æä¾› Wan2.1-T2V-1.3B æ–‡æœ¬åˆ°è§†é¢‘æ‰©æ•£æ¨¡å‹åœ¨ FP16 ä¸ W8A16ï¼ˆINT8 æƒé‡é‡åŒ–ï¼‰ æ¡ä»¶ä¸‹çš„å®Œæ•´å¤ç°æµç¨‹ï¼ŒåŒ…æ‹¬è§†é¢‘ç”Ÿæˆã€é‡åŒ–æ¨ç†ä»¥åŠä¸‰é¡¹å®¢è§‚è§†é¢‘è´¨é‡è¯„æµ‹æŒ‡æ ‡ã€‚

âœ¨ Features

ğŸ”¹ True INT8 weight-only quantization via TorchAO

ğŸ”¹ FP16 vs W8A16 paired video generation

ğŸ”¹ Three objective video metrics:

CLIP alignment

Temporal consistency

Motion magnitude (optical flow)

ğŸ”¹ Strict prompt/seed pairing for fair comparison

ğŸ”¹ Two-environment design (generation / evaluation)

ğŸ”¹ Paper-ready reproducibility workflow

ğŸ“ Project Structure
Wan2.1-main/
â”‚
â”œâ”€â”€ dataset/
â”‚   â”œâ”€â”€ fp16/
â”‚   â””â”€â”€ w8a16/
â”‚
â”œâ”€â”€ eval/
â”‚   â”œâ”€â”€ eval_motion.py
â”‚   â””â”€â”€ out/
â”‚
â”œâ”€â”€ eval_scripts/
â”‚   â””â”€â”€ eval_clip_temporal_simple.py
â”‚
â”œâ”€â”€ generate.py
â”œâ”€â”€ run_exp.py
â””â”€â”€ README.md

ğŸ”§ Environments

æœ¬é¡¹ç›®ä½¿ç”¨ä¸¤ä¸ª Conda ç¯å¢ƒï¼š

Environment	Usage
Wan1	è§†é¢‘ç”Ÿæˆ + é‡åŒ–æ¨ç†
clip_eval	è§†é¢‘è´¨é‡è¯„æµ‹
ğŸš€ Quick Start
1ï¸âƒ£ Activate Wan1 (Generation)
conda activate Wan1
cd /root/autodl-tmp/Wan2.1-main

2ï¸âƒ£ Generate Videos
FP16
USE_QUANT=0 python generate.py \
  --task t2v-1.3B \
  --ckpt_dir /root/autodl-tmp/Wan2.1-T2V-1.3B \
  --prompt "A cat playing guitar on the moon" \
  --size 832*480 \
  --frame_num 16 \
  --sample_steps 20 \
  --base_seed 123 \
  --offload_model True \
  --save_file dataset/fp16/01_seed123.mp4

W8A16
USE_QUANT=1 python generate.py \
  --task t2v-1.3B \
  --ckpt_dir /root/autodl-tmp/Wan2.1-T2V-1.3B \
  --prompt "A cat playing guitar on the moon" \
  --size 832*480 \
  --frame_num 16 \
  --sample_steps 20 \
  --base_seed 123 \
  --offload_model True \
  --save_file dataset/w8a16/01_seed123.mp4


Repeat for 20 prompts with identical seeds.

3ï¸âƒ£ Activate clip_eval (Evaluation)
conda activate clip_eval
cd /root/autodl-tmp/Wan2.1-main


Install dependencies:

pip install torch torchvision transformers pillow numpy pandas tqdm
pip install opencv-python imageio imageio-ffmpeg

ğŸ“Š Evaluation

Create output directory:

mkdir -p eval/out

CLIP Alignment + Temporal Consistency
python eval_scripts/eval_clip_temporal_simple.py \
  --fp16_dir dataset/fp16 \
  --w8a16_dir dataset/w8a16 \
  --out_dir eval/out


Outputs:

clip_temporal_per_video.csv

clip_temporal_summary.csv

Motion Magnitude (Optical Flow)
python eval/eval_motion.py \
  --fp16_dir dataset/fp16 \
  --w8a16_dir dataset/w8a16 \
  --out_dir eval/out


Output:

motion_summary.csv

ğŸ“ˆ Metrics Reported
Metric	Description
CLIP Score	Text-video semantic alignment
Temporal Consistency	Frame-level stability
Motion Magnitude	Optical flow strength

All metrics are computed on paired videos under identical inference settings.

ğŸ” Quantization Details

Backend: TorchAO

Mode: INT8 weight-only (W8A16)

Quantized: Transformer Linear layers

FP16 kept: Norm layers, embeddings, VAE

Offload: Enabled in both FP16 and W8A16

This ensures true quantization, not pseudo casting.

â™»ï¸ Reproducibility Rules

Same prompt & seed

Same resolution / frames / steps

Same offloading policy

Paired video comparison only

Metrics computed on real generated videos

ğŸ“š Citation

If you use this project, please cite:

Wan2.1-T2V-1.3B

OmniQuant

TorchAO

Video Diffusion Survey (Melnik et al.)

ğŸ“¬ Contact

Zhang Shenghe
City University of Macau
Email: D24091111148@cityu.edu.mo

â­ Acknowledgement

This project is intended as a reproducible engineering baseline for diffusion PTQ research on consumer GPUs.

End of README
